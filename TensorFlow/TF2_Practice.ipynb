{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce3b622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Way 2\n",
      "\n",
      "W=<tf.Variable 'Weight:0' shape=(1,) dtype=float32>, \n",
      "b=<tf.Variable 'Bias:0' shape=(1,) dtype=float32>, \n",
      "g=Tensor(\"gradient_tape/mul/Reshape:0\", shape=(1,), dtype=float32), \n",
      "g[1]=Tensor(\"gradient_tape/add/Reshape:0\", shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n",
    "import numpy as np\n",
    "tf1.disable_eager_execution()\n",
    "tf1.reset_default_graph()\n",
    "x_data = np.random.rand(100).astype('f')\n",
    "y_data = x_data * .1 + .3\n",
    "\n",
    "init = lambda : (tf.Variable(tf.random.uniform([1], -1., 1.), name=\"Weight\"),\n",
    "                 tf.Variable(tf.random.uniform([1], -1., 1.), name=\"Bias\"))\n",
    "W, b = init()\n",
    "yp = lambda x : W*x +b\n",
    "mse = lambda yp, yt : tf.reduce_mean(tf.square(yp - yt))\n",
    "\n",
    "way = 2\n",
    "if way == 1:\n",
    "    print(\"Way 1\")\n",
    "    for step in range(10):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = yp(x_data)\n",
    "            loss = mse(y, y_data)\n",
    "        grads = tape.gradient(loss, [W, b])\n",
    "        if step % 200 == 0:\n",
    "            print(f\"\\nW : {W.numpy()},\\nb = {b.numpy()},\\ng[0] : {grads[0].numpy()},\\ng[1] :{grads[1].numpy()}\")\n",
    "        W.assign(W - 0.01 * grads[0].numpy())\n",
    "        b.assign(b - 0.01 * grads[1].numpy())\n",
    "elif way == 2:\n",
    "    print(\"Way 2\")\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
    "    for step in range(10):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = yp(x_data)\n",
    "            loss = mse(y, y_data)\n",
    "        grads = tape.gradient(loss, [W, b])\n",
    "        process_grads = [g for g in grads]\n",
    "        grads_and_vars = zip(process_grads, [W, b])\n",
    "        tf1.summary.FileWriter(\"/tmp/lotto\", tf1.get_default_graph())\n",
    "        if step % 20 == 0:\n",
    "            print(f\"\\nW={W}, \\nb={b}, \\ng={grads[0]}, \\ng[1]={grads[1]}\")\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "elif way == 3:\n",
    "    print(\"Way 3\")\n",
    "    y_pred, yt = yp(x_data), y_data\n",
    "    optimzier = tf.optimizers.SGD(learning_rate=0.1)\n",
    "    for step in range(201):\n",
    "        if step % 20 == 0:\n",
    "            print(f\"\\nW = {W.numpy()}, \\nb = {b.numpy()}\")\n",
    "        mse_min = lambda : tf.reduce_mean(tf.square((W * x_data + b) - yt))\n",
    "        optimizer.minimize(mse_min, var_list=[W,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1fa897",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b45fbc5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data : (8, 3)\n",
      "y_data : (8, 3)\n",
      "step:0, grads:Tensor(\"AddN:0\", shape=(3, 3), dtype=float32), W:<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32>\n",
      "step:20, grads:Tensor(\"AddN_20:0\", shape=(3, 3), dtype=float32), W:<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32>\n",
      "step:40, grads:Tensor(\"AddN_40:0\", shape=(3, 3), dtype=float32), W:<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c752597ce099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/lotto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step:{step}, grads:{grads[0]}, W:{W}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, graph, max_queue, flush_secs, graph_def, filename_suffix, session)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, event_writer, graph, graph_def)\u001b[0m\n\u001b[1;32m     86\u001b[0m       \u001b[0;31m# graph may itself be a graph_def due to positional arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       maybe_graph_as_def = (graph.as_graph_def(add_shapes=True)\n\u001b[0;32m---> 88\u001b[0;31m                             if isinstance(graph, ops.Graph) else graph)\n\u001b[0m\u001b[1;32m     89\u001b[0m       self.add_meta_graph(\n\u001b[1;32m     90\u001b[0m           meta_graph.create_meta_graph_def(graph_def=graph_def or\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   3292\u001b[0m     \"\"\"\n\u001b[1;32m   3293\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3294\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   3209\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m       \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3211\u001b[0;31m       \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3212\u001b[0m       \u001b[0;31m# Strip the experimental library field iff it's empty.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/protobuf/message.py\u001b[0m in \u001b[0;36mParseFromString\u001b[0;34m(self, serialized)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \"\"\"\n\u001b[1;32m    198\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mMergeFromString\u001b[0;34m(self, serialized)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m         \u001b[0;31m# The only reason _InternalParse would return early is if it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# encountered an end-group tag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mInternalParse\u001b[0;34m(self, buffer, pos, end)\u001b[0m\n\u001b[1;32m   1204\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfield_desc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_UpdateOneofState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/protobuf/internal/decoder.py\u001b[0m in \u001b[0;36mDecodeRepeatedField\u001b[0;34m(buffer, pos, end, message, field_dict)\u001b[0m\n\u001b[1;32m    731\u001b[0m           \u001b[0;32mraise\u001b[0m \u001b[0m_DecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Truncated message.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;31m# Read sub-message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m           \u001b[0;31m# The only reason _InternalParse would return early is if it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m           \u001b[0;31m# encountered an end-group tag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mInternalParse\u001b[0;34m(self, buffer, pos, end)\u001b[0m\n\u001b[1;32m   1204\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfield_desc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_UpdateOneofState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/protobuf/internal/decoder.py\u001b[0m in \u001b[0;36mDecodeMap\u001b[0;34m(buffer, pos, end, message, field_dict)\u001b[0m\n\u001b[1;32m    886\u001b[0m       \u001b[0;31m# Read sub-message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0msubmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0msubmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m         \u001b[0;31m# The only reason _InternalParse would return early is if it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0;31m# encountered an end-group tag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mInternalParse\u001b[0;34m(self, buffer, pos, end)\u001b[0m\n\u001b[1;32m   1153\u001b[0m   \u001b[0mdecoders_by_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoders_by_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mInternalParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m     \"\"\"Create a message from serialized bytes.\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n",
    "import numpy as np\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "# tf1.enable_eager_execution()\n",
    "tf1.disable_eager_execution()\n",
    "tf1.reset_default_graph()\n",
    "\n",
    "xy = np.loadtxt(\"softmax.csv\", unpack=True, delimiter=',', dtype=\"float32\")\n",
    "x_data = np.transpose(xy[:3])\n",
    "y_data = np.transpose(xy[3:])\n",
    "print(f\"x_data : {x_data.shape}\")\n",
    "print(f\"y_data : {y_data.shape}\")\n",
    "\n",
    "W = tf.Variable(tf.zeros([3, 3]))\n",
    "hx = lambda X: tf.nn.softmax(tf.matmul(X, W))\n",
    "\n",
    "cost = lambda Y: tf.reduce_mean(-tf.reduce_sum(Y * tf.math.log(hx(x_data)), 1))\n",
    "rate = tf.Variable(.01)\n",
    "\n",
    "opt = tf.optimizers.SGD(rate)\n",
    "for step in range(2001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        L1_reg = 0.1 * tf.reduce_sum(tf.abs(W))\n",
    "        loss = cost(y_data) + L1_reg\n",
    "    grads = tape.gradient(loss, [W])\n",
    "    opt.apply_gradients(zip(grads, [W]))\n",
    "    \n",
    "    tf1.summary.FileWriter(\"/tmp/lotto\", tf1.get_default_graph())\n",
    "    if step % 20 == 0:\n",
    "        print(f\"step:{step}, grads:{grads[0]}, W:{W}\") \n",
    "    \n",
    "print('-'*60)\n",
    "a = hx([[1., 11., 7.]]).numpy()\n",
    "b = hx([[1., 3., 4.]]).numpy()\n",
    "c = hx([[1., 1., 0.]]).numpy()\n",
    "abc = hx([[1., 11, 7], [1., 3., 4.], [1., 1., 0.]])\n",
    "print(f\"[1., 11., 7.] : {a}, {tf.argmax(a, 1).numpy()}\")\n",
    "print(f\"[1., 3., 4.] : {b}, {tf.argmax(b, 1).numpy()}\")\n",
    "print(f\"[1., 1., 0.] : {c}, {tf.argmax(c, 1).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6244c7",
   "metadata": {},
   "source": [
    "# XOR TF1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b90ab819",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data : \n",
      "[[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]], \n",
      "y_data : \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tf.placeholder() is not compatible with eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a1261423c04a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"x_data : \\n{x_data}, \\ny_data : \\n{y_data}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   3095\u001b[0m   \"\"\"\n\u001b[1;32m   3096\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3097\u001b[0;31m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[1;32m   3098\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   3099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf1.disable_eager_execution()\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt(\"xor.csv\", unpack=True, delimiter=',')\n",
    "\n",
    "x_data = np.transpose(xy[:-1])\n",
    "y_data = np.reshape(xy[-1], (4, 1))\n",
    "\n",
    "print(f\"x_data : \\n{x_data}, \\ny_data : \\n{y_data}\")\n",
    "\n",
    "X = tf1.placeholder(tf.float32)\n",
    "Y = tf1.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf1.Variable(tf.random.uniform([2, 2], -1.0, 1.0))\n",
    "W2 = tf1.Variable(tf.random.uniform([2, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf1.Variable(tf.zeros([2]))\n",
    "b2 = tf1.Variable(tf.zeros([1]))\n",
    "\n",
    "L2 = tf1.sigmoid(tf.matmul(X, W1) + b1)\n",
    "hypothsis = tf1.sigmoid(tf1.matmul(L2, W2) + b2)\n",
    "\n",
    "cost = -tf1.reduce_mean(Y * tf1.log(hypothsis) + (1-Y) * tf1.log(1-hypothsis))\n",
    "\n",
    "rate = tf1.Variable(0.1)\n",
    "optimizer = tf1.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf1.global_variables_initializer()\n",
    "\n",
    "with tf1.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(10000):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 999:\n",
    "            r1, (r2, r3) = sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2])\n",
    "            print(f\"step: {step}, {r1}, {np.reshape(r2, (1,4)), np.reshape(r3, (1,2))}\")\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc25ab2",
   "metadata": {},
   "source": [
    "# XOR TF2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a914ba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter :0, loss :0.7052617073059082\n",
      "Iter :500, loss :0.37564024329185486\n",
      "Iter :1000, loss :0.3546944260597229\n",
      "Iter :1500, loss :0.3503865599632263\n",
      "Iter :2000, loss :0.3487494885921478\n",
      "Iter :2500, loss :0.3479474186897278\n",
      "Iter :3000, loss :0.347495436668396\n",
      "Iter :3500, loss :0.34721702337265015\n",
      "Iter :4000, loss :0.34703508019447327\n",
      "Iter :4500, loss :0.34691092371940613\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt(\"xor.csv\", delimiter=',', unpack=True, dtype='f')\n",
    "x_data = xy[:-1].T\n",
    "y_data = np.reshape(xy[-1], (4, 1))\n",
    "dataset = \\\n",
    "tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n",
    "\n",
    "def preprocess_data(features, labels):\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    lables = tf.cast(labels, tf.float32)\n",
    "    return features, labels\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal((2, 2)), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random.normal((2,)), name=\"bias1\")\n",
    "W2 = tf.Variable(tf.random.normal((2, 1)), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random.normal((1,), name=\"bias2\"))\n",
    "\n",
    "def neural_net(features):\n",
    "    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) +b2)\n",
    "    return hypothesis\n",
    "\n",
    "def loss_fn(hypothesis, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "\n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(neural_net(features), labels)\n",
    "    return tape.gradient(loss_value, [W1, W2, b1, b2])\n",
    "\n",
    "EPOCHS = 5000\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels in dataset:\n",
    "        featuers, labels = preprocess_data(features, labels)\n",
    "        grads = grad(neural_net(features), features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, [W1, W2, b1, b2]))\n",
    "        \n",
    "    if step % 500 == 0:\n",
    "        print(f\"Iter :{step}, loss :{loss_fn(neural_net(features), labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82fd4902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_prediction = tf.cast(neural_net(features) > .5,\n",
    "                            dtype = tf.float32)\n",
    "\n",
    "accuracy_fn(neural_net(features), labels).numpy()\n",
    "correct_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4c48a",
   "metadata": {},
   "source": [
    "# SoftMax 도 Binary 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b21d3cd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 2.7728\n",
      "1 : 2.7728\n",
      "2 : 2.7724\n",
      "3 : 2.7715\n",
      "4 : 2.7692\n",
      "5 : 2.7653\n",
      "6 : 2.7603\n",
      "7 : 2.7565\n",
      "8 : 2.7509\n",
      "9 : 2.7449\n",
      "10 : 2.7391\n",
      "11 : 2.7301\n",
      "12 : 2.7213\n",
      "13 : 2.7116\n",
      "14 : 2.7016\n",
      "15 : 2.6905\n",
      "16 : 2.6782\n",
      "17 : 2.6633\n",
      "18 : 2.6481\n",
      "19 : 2.6361\n",
      "20 : 2.6158\n",
      "21 : 2.5958\n",
      "22 : 2.5751\n",
      "23 : 2.5497\n",
      "24 : 2.5197\n",
      "25 : 2.4911\n",
      "26 : 2.4525\n",
      "27 : 2.4162\n",
      "28 : 2.3861\n",
      "29 : 2.3435\n",
      "30 : 2.2938\n",
      "31 : 2.2468\n",
      "32 : 2.1949\n",
      "33 : 2.1403\n",
      "34 : 2.0805\n",
      "35 : 2.0286\n",
      "36 : 1.9647\n",
      "37 : 1.9080\n",
      "38 : 1.8455\n",
      "39 : 1.7837\n",
      "40 : 1.7190\n",
      "41 : 1.6535\n",
      "42 : 1.5881\n",
      "43 : 1.5249\n",
      "44 : 1.4559\n",
      "45 : 1.3872\n",
      "46 : 1.3243\n",
      "47 : 1.2600\n",
      "48 : 1.1976\n",
      "49 : 1.1359\n",
      "50 : 1.0748\n",
      "51 : 1.0152\n",
      "------------------------------------------------------------\n",
      "accruacy: 100.00%\n",
      "Tensor(\"ArgMax_10:0\", shape=(None,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_eager_execution()\n",
    "sess = tf1.InteractiveSession()\n",
    "\n",
    "x_ = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "expect = [[1, 0], [0, 1], [0, 1], [1, 0]]\n",
    "x = tf1.placeholder(\"float\", [None, 2])\n",
    "y_ = tf1.placeholder(\"float\", [None, 2])\n",
    "\n",
    "num_hidden = 20\n",
    "\n",
    "W = tf.Variable(tf.random.uniform([2, num_hidden], -.01, .01))\n",
    "b = tf.Variable(tf.random.uniform([num_hidden], -.01, .01))\n",
    "\n",
    "hidden = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "\n",
    "W2 = tf.Variable(tf.random.uniform([num_hidden, 2], -.01, .01))\n",
    "b2 = tf.Variable(tf.random.uniform([2], -.01, .01))\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(hidden, W2) + b2)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.math.log(y))\n",
    "# train_step = tf1.train.GradientDescentOptimizer(.2).minimize(cross_entropy)\n",
    "train_step = tf1.train.AdamOptimizer(.02).minimize(cross_entropy)\n",
    "\n",
    "tf1.global_variables_initializer().run()\n",
    "\n",
    "for step in range(1000):\n",
    "    feed = {x: x_, y_: expect}\n",
    "    e, a = sess.run([cross_entropy, train_step], feed)\n",
    "    if e < 1: break # Early Stopping\n",
    "    print(f\"{step} : {e:.4f}\")\n",
    "print('-'*60)\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print(f\"accruacy: {accuracy.eval(feed)*100:.2f}%\")\n",
    "learned_output = tf.argmax(y, 1)\n",
    "print(learned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13c3de62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, cost : 43072.507978\n",
      "Epoch : 2, cost : 10495.201650\n",
      "Epoch : 3, cost : 6577.473928\n",
      "Epoch : 4, cost : 4544.423878\n",
      "Epoch : 5, cost : 3272.067510\n",
      "Epoch : 6, cost : 2389.454336\n",
      "Epoch : 7, cost : 1786.702470\n",
      "Epoch : 8, cost : 1330.298726\n",
      "Epoch : 9, cost : 980.887135\n",
      "Epoch : 10, cost : 737.370490\n",
      "Epoch : 11, cost : 580.988824\n",
      "Epoch : 12, cost : 436.568086\n",
      "Epoch : 13, cost : 354.536136\n",
      "Epoch : 14, cost : 247.551312\n",
      "Epoch : 15, cost : 213.671548\n",
      "Optimization Finished!\n",
      "Accuracy : 94.76\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_eager_execution()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train.reshape(-1, 784), x_test.reshape(-1, 784)\n",
    "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10) # one hot encoding 주의\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "X = tf1.placeholder(tf.float32, [None, 784])\n",
    "Y = tf1.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([784, 256]))\n",
    "B1 = tf.Variable(tf.random.normal([256]))\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([256, 256]))\n",
    "B2 = tf.Variable(tf.random.normal([256]))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), B2))\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([256, 10]))\n",
    "B3 = tf.Variable(tf.random.normal([10]))\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), B3)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf1.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf1.global_variables_initializer()\n",
    "with tf1.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        training_batch = zip(range(0, len(x_train), batch_size), range(batch_size, len(x_train)+1, batch_size))\n",
    "        for start, end in training_batch:\n",
    "            batch_xs, batch_ys = x_train[start:end], y_train[start:end]\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / int(len(y_train)/batch_size)\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(f\"Epoch : {epoch+1}, cost : {avg_cost:4f}\")\n",
    "                \n",
    "    print(\"Optimization Finished!\")\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(f\"Accuracy : {accuracy.eval({X:x_test, Y:y_test})*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0803ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
